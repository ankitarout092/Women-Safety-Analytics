{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701636a1-62f7-4241-8883-74e6c77f130b",
   "metadata": {},
   "source": [
    "# Dataset: https://www.kaggle.com/datasets/mohamedmustafa/real-life-violence-situations-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f236c3-c710-4fb1-93b9-04538d715777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Epoch 1/50\n",
      "123/123 [==============================] - 13s 50ms/step - loss: 0.7164 - accuracy: 0.6857 - val_loss: 0.9728 - val_accuracy: 0.7000\n",
      "Epoch 2/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.5340 - accuracy: 0.7796 - val_loss: 5.2432 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.5623 - accuracy: 0.7490 - val_loss: 4.9741 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.4335 - accuracy: 0.8082 - val_loss: 7.8887 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "123/123 [==============================] - 5s 41ms/step - loss: 0.4564 - accuracy: 0.8143 - val_loss: 4.2900 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "123/123 [==============================] - 5s 41ms/step - loss: 0.3610 - accuracy: 0.8347 - val_loss: 4.5787 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.3198 - accuracy: 0.8776 - val_loss: 7.3385 - val_accuracy: 0.5048\n",
      "Epoch 8/50\n",
      "123/123 [==============================] - 6s 52ms/step - loss: 0.3501 - accuracy: 0.8571 - val_loss: 2.9140 - val_accuracy: 0.5524\n",
      "Epoch 9/50\n",
      "123/123 [==============================] - 5s 41ms/step - loss: 0.2603 - accuracy: 0.9041 - val_loss: 4.4121 - val_accuracy: 0.5667\n",
      "Epoch 10/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.2024 - accuracy: 0.9224 - val_loss: 5.3816 - val_accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.3226 - accuracy: 0.8755 - val_loss: 2.4151 - val_accuracy: 0.5238\n",
      "Epoch 12/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.2160 - accuracy: 0.9245 - val_loss: 5.9357 - val_accuracy: 0.5190\n",
      "Epoch 13/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.2281 - accuracy: 0.9143 - val_loss: 1.4777 - val_accuracy: 0.5571\n",
      "Epoch 14/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.1700 - accuracy: 0.9347 - val_loss: 2.0758 - val_accuracy: 0.5619\n",
      "Epoch 15/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.2124 - accuracy: 0.9204 - val_loss: 2.7032 - val_accuracy: 0.6333\n",
      "Epoch 16/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.1642 - accuracy: 0.9408 - val_loss: 1.8319 - val_accuracy: 0.6000\n",
      "Epoch 17/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0910 - accuracy: 0.9673 - val_loss: 1.0483 - val_accuracy: 0.6905\n",
      "Epoch 18/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0283 - accuracy: 0.9898 - val_loss: 2.8231 - val_accuracy: 0.5476\n",
      "Epoch 19/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.2049 - accuracy: 0.9245 - val_loss: 1.0778 - val_accuracy: 0.5857\n",
      "Epoch 20/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.1387 - accuracy: 0.9469 - val_loss: 2.6521 - val_accuracy: 0.5619\n",
      "Epoch 21/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0784 - accuracy: 0.9755 - val_loss: 0.6920 - val_accuracy: 0.8000\n",
      "Epoch 22/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0686 - accuracy: 0.9857 - val_loss: 0.5262 - val_accuracy: 0.8095\n",
      "Epoch 23/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0493 - accuracy: 0.9735 - val_loss: 1.1619 - val_accuracy: 0.6762\n",
      "Epoch 24/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.2241 - accuracy: 0.9082 - val_loss: 0.5140 - val_accuracy: 0.7524\n",
      "Epoch 25/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0689 - accuracy: 0.9796 - val_loss: 0.6932 - val_accuracy: 0.8095\n",
      "Epoch 26/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0519 - accuracy: 0.9878 - val_loss: 1.8790 - val_accuracy: 0.6429\n",
      "Epoch 27/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0503 - accuracy: 0.9796 - val_loss: 0.7047 - val_accuracy: 0.7619\n",
      "Epoch 28/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0160 - accuracy: 0.9959 - val_loss: 1.2231 - val_accuracy: 0.7762\n",
      "Epoch 29/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.2338 - val_accuracy: 0.7857\n",
      "Epoch 30/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0106 - accuracy: 0.9959 - val_loss: 1.3705 - val_accuracy: 0.7762\n",
      "Epoch 31/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.2804 - accuracy: 0.9122 - val_loss: 2.1264 - val_accuracy: 0.5524\n",
      "Epoch 32/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.1264 - accuracy: 0.9612 - val_loss: 3.2297 - val_accuracy: 0.5667\n",
      "Epoch 33/50\n",
      "123/123 [==============================] - 5s 41ms/step - loss: 0.0971 - accuracy: 0.9633 - val_loss: 4.1058 - val_accuracy: 0.5048\n",
      "Epoch 34/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0434 - accuracy: 0.9857 - val_loss: 1.5537 - val_accuracy: 0.7048\n",
      "Epoch 35/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.1762 - accuracy: 0.9388 - val_loss: 5.6886 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0907 - accuracy: 0.9653 - val_loss: 1.9679 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.1293 - accuracy: 0.9551 - val_loss: 8.5123 - val_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "123/123 [==============================] - 5s 43ms/step - loss: 0.0373 - accuracy: 0.9878 - val_loss: 7.3972 - val_accuracy: 0.5000\n",
      "Epoch 39/50\n",
      "123/123 [==============================] - 5s 43ms/step - loss: 0.0207 - accuracy: 0.9959 - val_loss: 5.8238 - val_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 6.2919 - val_accuracy: 0.5000\n",
      "Epoch 41/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.1259 - accuracy: 0.9592 - val_loss: 2.0675 - val_accuracy: 0.4667\n",
      "Epoch 42/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.2420 - accuracy: 0.9102 - val_loss: 2.3616 - val_accuracy: 0.5000\n",
      "Epoch 43/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0365 - accuracy: 0.9898 - val_loss: 7.6149 - val_accuracy: 0.5000\n",
      "Epoch 44/50\n",
      "123/123 [==============================] - 5s 43ms/step - loss: 0.0265 - accuracy: 0.9959 - val_loss: 4.2673 - val_accuracy: 0.5000\n",
      "Epoch 45/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.1454 - accuracy: 0.9469 - val_loss: 2.1668 - val_accuracy: 0.5286\n",
      "Epoch 46/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0506 - accuracy: 0.9796 - val_loss: 1.8523 - val_accuracy: 0.6810\n",
      "Epoch 47/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0116 - accuracy: 0.9980 - val_loss: 1.4442 - val_accuracy: 0.7524\n",
      "Epoch 48/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.1450 - val_accuracy: 0.7286\n",
      "Epoch 49/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 7.2969e-04 - accuracy: 1.0000 - val_loss: 0.8994 - val_accuracy: 0.8095\n",
      "Epoch 50/50\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 8.9661e-04 - accuracy: 1.0000 - val_loss: 0.9088 - val_accuracy: 0.8238\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set image size and color channels\n",
    "IMG_SIZE = 128\n",
    "ColorChannels = 3\n",
    "\n",
    "# Function to convert video to frames with augmentation\n",
    "def video_to_frames(video):\n",
    "    vidcap = cv2.VideoCapture(video)\n",
    "    ImageFrames = []\n",
    "    while vidcap.isOpened():\n",
    "        ID = vidcap.get(1)\n",
    "        success, image = vidcap.read()\n",
    "        if success and (ID % 7 == 0):\n",
    "            resized = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), (IMG_SIZE, IMG_SIZE))\n",
    "            ImageFrames.append(resized)\n",
    "        else:\n",
    "            break\n",
    "    vidcap.release()\n",
    "    return ImageFrames\n",
    "\n",
    "# Paths to video data\n",
    "PROJECT_DIR = 'D:/Extract'\n",
    "VideoDataDir = os.path.join(PROJECT_DIR, 'Real Life Violence Dataset')\n",
    "\n",
    "# Initialize lists to store data\n",
    "X_original = []\n",
    "y_original = []\n",
    "\n",
    "# Select classes and limit the number of videos to process\n",
    "CLASSES = [\"NonViolence\", \"Violence\"]\n",
    "VIDEO_LIMIT = 350\n",
    "\n",
    "# Process videos and extract frames\n",
    "for category in CLASSES:\n",
    "    path = os.path.join(VideoDataDir, category)\n",
    "    class_num = CLASSES.index(category)\n",
    "    for video in os.listdir(path)[:VIDEO_LIMIT]:\n",
    "        frames = video_to_frames(os.path.join(path, video))\n",
    "        X_original.extend(frames)\n",
    "        y_original.extend([class_num] * len(frames))\n",
    "\n",
    "# Convert lists to NumPy arrays and reshape\n",
    "X_original = np.array(X_original).reshape(-1, IMG_SIZE * IMG_SIZE * 3)\n",
    "y_original = np.array(y_original)\n",
    "\n",
    "# Split data into training and test sets\n",
    "stratified_sample = StratifiedShuffleSplit(n_splits=2, test_size=0.3, random_state=73)\n",
    "for train_index, test_index in stratified_sample.split(X_original, y_original):\n",
    "    X_train, X_test = X_original[train_index], X_original[test_index]\n",
    "    y_train, y_test = y_original[train_index], y_original[test_index]\n",
    "\n",
    "# Reshape data for neural network input\n",
    "X_train_nn = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, 3) / 255\n",
    "X_test_nn = X_test.reshape(-1, IMG_SIZE, IMG_SIZE, 3) / 255\n",
    "\n",
    "# Load MobileNetV2 model and compile\n",
    "def load_layers():\n",
    "    input_tensor = Input(shape=(IMG_SIZE, IMG_SIZE, ColorChannels))\n",
    "    baseModel = MobileNetV2(pooling='avg', include_top=False, input_tensor=input_tensor)\n",
    "    headModel = Dense(1, activation=\"sigmoid\")(baseModel.output)\n",
    "    model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = load_layers()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_nn, y_train, epochs=50, validation_data=(X_test_nn, y_test), batch_size=4)\n",
    "\n",
    "# Save final model\n",
    "model.save(\"final_model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
